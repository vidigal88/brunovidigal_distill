[
  {
    "path": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/",
    "title": "Web Scraping Brazil's Presidential Election Data",
    "description": "This post shows how to scrape Brazil's Presidential Election Data from TSE.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {}
      }
    ],
    "date": "2021-01-09",
    "categories": [],
    "contents": "\n\nContents\nMotivation\nView page source\nR codeWeb scraping\nRead in data\n\nConclusion\n\nMotivation\nI won’t lie to you. The very first time I downloaded elections data from the Tribunal Superior Eleitoral TSE was manually … and painful.\n28 files to download and unzip … MANUALLY!\nBut it was also clear to me that this procedure wasn’t the most recommended. I should figure out a better way to do that.\nThus, looking for a simple way to solve this problem I found this question on stackoverflow:\n\n\n\nAnd Hadley answered it with class.\n\n\n\nWith this valuable info, I needed to:\nFind the downloadable link as per the question;\nApply&Adapt Hadley’s solution using rvest package.\nView page source\nIn order to find the downloadable link, see the gif below or check this youtube video as the resolution is higher. The tip is to look for the downloadable link right clicking on the View page source button.\n\n\n\nR code\nThe R packages used are:\n\n\nlibrary(rvest)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(stringr)\n\n\n\nWeb scraping\nThe R code below is pretty much what Hadley had posted. I have just adapted it to my problem. Then the object page receives the html file we saw in the video above. The function read_html() from package xml2 just reads the html. After that we apply html_nodes() to find the links, html_attr() to get the url and str_subset() to find the files ending in .zip and excluding the ones ending in .sha.\n\n\npage <- xml2::read_html(\"https://www.tse.jus.br/hotsites/pesquisas-eleitorais/resultados_anos/boletim_urna/2018/boletim_urna_2_turno.html\")\n\nzip_files <- page %>%\n  html_nodes(\"a\") %>%       # find all links\n  html_attr(\"href\") %>%     # get the url\n  str_subset(\"\\\\.zip\") %>% # find those that end in .zip\n  str_subset(\"\\\\.sha\", negate = TRUE) # rid of the ones ending in .sha\n\n\n\nOnce you have run this code above, you download those files, unzip and save them in your machine.\n\n\nfor(i in seq_along(zip_files)) {\n  \n  temp <- tempfile()\n  download.file(zip_files[i], temp)\n  unzip(temp, exdir = \"data/elections_2018\")\n  unlink(temp)\n  \n}\n\n\n\nRead in data\nAs we are lazy (or should I say smart enough), let’s list all data at once with the function list.files().\n\n\ncsvs_to_read = list.files(\n  path = \"data/elections_2018\",  \n  pattern = \".*(bweb_2t).*csv$\", \n  recursive = TRUE,          \n  full.names = TRUE         \n)\n\n\n\nThat done, you can use the fantastic R function map_df() from purrr coupled with fread() from data.table.\n\n\nelections_2nd_round <- \n  csvs_to_read %>% \n  map_df(~fread(., colClasses = 'character', showProgress = TRUE)) %>% \n  as_tibble()\n\n\n\nIn a few seconds you get your data (nearly 3 million rows and 1,3GB) ready to be analysed.\nConclusion\nThat’s all folks. Pretty simple to web scraping Brazil’s Presidential Election data.\n\n\n\n",
    "preview": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/images/stackoverflow_question.png",
    "last_modified": "2021-01-10T18:27:52+01:00",
    "input_file": {}
  }
]
