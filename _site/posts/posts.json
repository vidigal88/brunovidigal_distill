[
  {
    "path": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/",
    "title": "Web Scraping Brazil's Presidential Election Data",
    "description": "This post shows how to scrape Brazil's Presidential Election Data from TSE.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {}
      }
    ],
    "date": "2021-01-09",
    "categories": [],
    "contents": "\n\nContents\nMotivation\nView page source\nR codeWeb scraping\nRead in data\n\nConclusion\n\nMotivation\nI won’t lie to you. The very first time I downloaded elections data from the Tribunal Superior Eleitoral TSE was manually … and painful.\n28 files to download and unzip … MANUALLY!\nBut it was also clear to me that this procedure wasn’t the most recommended. I should figure out a better way to do that.\nThus, looking for a simple way to solve this problem I found this question on stackoverflow:\n\n\n\nAnd Hadley answered it with class.\n\n\n\nWith this valuable info, I needed to:\nFind the downloadable link as per the question;\nApply&Adapt Hadley’s solution using rvest package.\nView page source\nIn order to find the downloadable link, see the gif below or check this youtube video as the resolution is higher. The tip is to look for the downloadable link right clicking on the View page source button.\n\n\n\nR code\nThe R packages used are:\n\n\nlibrary(rvest)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(stringr)\n\n\n\nWeb scraping\nThe R code below is pretty much what Hadley had posted. I have just adapted it to my problem. Then the object page receives the html file we saw in the video above. The function read_html() from package xml2 just reads the html. After that we apply html_nodes() to find the links, html_attr() to get the url and str_subset() to find the files ending in .zip and excluding the ones ending in .sha.\n\n\npage <- xml2::read_html(\"https://www.tse.jus.br/hotsites/pesquisas-eleitorais/resultados_anos/boletim_urna/2018/boletim_urna_2_turno.html\")\n\nzip_files <- page %>%\n  html_nodes(\"a\") %>%       # find all links\n  html_attr(\"href\") %>%     # get the url\n  str_subset(\"\\\\.zip\") %>% # find those that end in .zip\n  str_subset(\"\\\\.sha\", negate = TRUE) # rid of the ones ending in .sha\n\n\n\nOnce you have run this code above, you download those files, unzip and save them in your machine.\n\n\nfor(i in seq_along(zip_files)) {\n  \n  temp <- tempfile()\n  download.file(zip_files[i], temp)\n  unzip(temp, exdir = \"data/elections_2018\")\n  unlink(temp)\n  \n}\n\n\n\nRead in data\nAs we are lazy (or should I say smart enough), let’s list all data at once with the function list.files().\n\n\ncsvs_to_read = list.files(\n  path = \"data/elections_2018\",  \n  pattern = \".*(bweb_2t).*csv$\", \n  recursive = TRUE,          \n  full.names = TRUE         \n)\n\n\n\nThat done, you can use the fantastic R function map_df() from purrr coupled with fread() from data.table.\n\n\nelections_2nd_round <- \n  csvs_to_read %>% \n  map_df(~fread(., colClasses = 'character', showProgress = TRUE)) %>% \n  as_tibble()\n\n\n\nIn a few seconds you get your data (nearly 3 million rows and 1,3GB) ready to be analysed.\nConclusion\nThat’s all folks. Pretty simple to web scraping Brazil’s Presidential Election data.\n\n\n\n",
    "preview": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/images/stackoverflow_question.png",
    "last_modified": "2021-01-10T18:27:52+01:00",
    "input_file": {},
    "preview_width": 957,
    "preview_height": 914
  },
  {
    "path": "posts/2021-01-10-premier-league-twitter-data-visualization-with-r/",
    "title": "Premier League: Twitter Data Visualization with R",
    "description": "From tweets to data visualization in R.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {
          "www.brunovidigal.com": {}
        }
      }
    ],
    "date": "2020-10-26",
    "categories": [],
    "contents": "\n\nContents\nSynopsis\nData collection: rtweet\nData Manipulation\nData Visualization\n\nSynopsis\nLiverpool and Everton met on Saturday 17 October 2020 for the fifth round of the English Premier League (EPL), the most competitive and thrilling national tournament of the world - it’s just the best. As a good and traditional derby, the teams brought a lot of rivalry to the pitch and as result, a 2-2 draw, full of polemics and drama. As I am passionate about both statistics and football/soccer (the beautiful game), I decided to analyze what was going on on Twitter during the match. To do so, I used the great R package rtweet to pull tweets with the hashtags #EVELIV and #MerseySideDerby.\nData collection: rtweet\nThere are many tutorials on the web about how to pull Twitter data with rtweet. For instance, you can have a look at:\ntowards data science\nEarth Data Science\nMedium\nAs there are many good articles on Internet, I will assume you know how to get your Twitter API access and will just show you the parameters used to get the tweets.\nFirstly, you you’ll need to load some R packages.\n\n\n# load packages -----------------------------------------------------------\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(magick)\n\n\n\nAfter loading the packages you will need to specify the authentication token’s provided in your Twitter App with the function create_token(). After that you can use the function search_tweets() to get your data.\n\n\ncreate_token(\n  app = \"Liverpool tweets\",\n  consumer_key = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  consumer_secret = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  access_token = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  access_secret = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\ntweets <- search_tweets(\"EVELIV OR MerseysideDerby\", n = 100000, \n                                    include_rts = FALSE, lang = \"en\", retryonratelimit = TRUE)\n\n\n\nData Manipulation\nAlthough I have put n equals to 100,000, I got about 40k rows (39982), which I assume is the number of tweets with these hashtags. Another step towards our aim is to filter out some columns. The API gives you 90 columns. In this exercise we’re interested in some of them.\n\n\ntweets <- tweets %>% select(user_id, status_id, created_at, screen_name, text, \n                            display_text_width, is_quote, is_retweet, favorite_count,\n                            retweet_count, quote_count, hashtags, mentions_screen_name)\n\n\n\nBear in mind that the time shown in the Twitter API is set to UTC. The game took place in Liverpool (GMT +1) and started at 12:30 pm (local time), 11:30 am (UTC). We will consider the first and the last tweet of the Liverpool Football Club as the start (11:31:49 am UTC) and the end (13:28:15 UTC) time of the match. Using mutate() and case_when() we create a new variable moment with three categories: pre-game, game and post-game. Here we’re interested in just the moment of the match.\n\n\ngame_started_at <- as.POSIXct('2020-10-17 11:31:49', tz = 'UTC')\ngame_ended_at <- as.POSIXct('2020-10-17 13:28:15', tz = 'UTC')\n\ntweets <- tweets %>% \n  mutate(\n    moment = case_when(\n      created_at >= game_started_at & created_at <= game_ended_at ~ \"game\",\n      created_at > game_ended_at ~ \"post-game\", \n      TRUE ~ \"pre-game\"))\n\n\n\nData Visualization\nWe load the Premier League and Twitter logos to make the graph looks nicer. In this exercise I am using the package magick, function image_read().\n\n\npremier_league_logo = image_read(\"premier_league_logo.jpg\")\ntwitter_logo <- image_read(\"Twitter_bird_logo.png\")\n\n\n\nNow we’re ready to plot the time series of the number of tweets with the hashtags #EVERLIV and #MerseySideDerby during the match. However, let’s first create the graphic without customizing it. Thus, we’ll see how good is the graphic with the proper logos and colors. This is the best way to value how important is customization. As statisticians/data analysts/data scientists, we need to know how to present a good graphic to an end user/client.\n\n\n  tweets %>% \n  filter(moment == \"game\") %>%\n  ggplot(aes(created_at)) + \n  geom_freqpoly(binwidth = 60) \n\n\n\n\nAnd now we make the same graphic adding the Premier League colors, logos and the main events with annotate(). Go check also the Michael Toth blog for more tips on how to brand your graphs.\n\n\nsource('theme_epl.R') # customize theme for English Premier League\n\n  tweets %>% \n  filter(moment == \"game\") %>%\n  ggplot(aes(created_at)) + \n  geom_freqpoly(binwidth = 60, size = 1.2, col = '#3d195b') +\n  xlab('Time of the match - UTC') + ylab('') +  \n  scale_y_continuous(labels = comma) +\n  labs(title = \"#tweets (#EVELIV and #MerseysideDerby) over Everton vs Liverpool\", \n       subtitle = \"English Premier League - 5th round, 17th October 2020\",\n       caption = \"@vidigal_br\", \n       family = \"sans\") + \n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), y = 1100, \n           label = \"1st Goal \\n Sadio Mane\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:42:00\", tz = 'UTC'), y = 30, \n           label = \"Virgil van Dijk \\n replaced\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), y = 1100, \n           label = \"1st Goal \\n Michael Keane\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), y = 800, \n           label = \"2nd Goal \\n Salah\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), y = 1100, \n           label = \"2nd Goal \\n Calvert-Lewin\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), y = 30, \n           label = \"Red card \\n Richarlisson\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), y = 1650, \n           label = \"VAR: NO GOAL \\n Offside\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 12:28:00\", tz = 'UTC'), y = 800, \n           label = \"Interval\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:55:00\", tz = 'UTC'), y = 1600, \n           label = \"1st Half\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:05:00\", tz = 'UTC'), y = 1600, \n           label = \"2nd Half\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), y = 1000, \n    xend = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), yend = 450, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:40:00\", tz = 'UTC'), y = 100,\n    xend = as.POSIXct(\"2020-10-17 11:40:00\", tz = 'UTC'), yend = 300, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), y = 1000,\n    xend = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), yend = 480, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), y = 650, \n    xend = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), yend = 480, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), y = 950,\n    xend = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), yend = 650, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), y = 150,\n    xend = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), yend = 300, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), y = 1550,\n    xend = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), yend = 1450, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) + \n  geom_vline(xintercept = as.POSIXct(\"2020-10-17 12:21:08\", tz = 'UTC'), linetype=\"dotted\", \n             color = \"#3d195b\", size=1.5) + \n  geom_vline(xintercept = as.POSIXct(\"2020-10-17 12:36:07\", tz = 'UTC'), linetype=\"dotted\", \n             color = \"#3d195b\", size=1.5) +\n  annotation_custom(rasterGrob(premier_league_logo),\n                    xmin = as.POSIXct(\"2020-10-17 11:15:00\", tz = 'UTC'), \n                    xmax = as.POSIXct(\"2020-10-17 11:25:00\", tz = 'UTC'), \n                    ymin = -100, ymax = -250) +\n  annotation_custom(rasterGrob(twitter_logo),\n                    xmin = as.POSIXct(\"2020-10-17 13:20:00\", tz = 'UTC'), \n                    xmax = as.POSIXct(\"2020-10-17 13:25:00\", tz = 'UTC'), \n                    ymin = -250, ymax = -120) +\n  coord_cartesian(clip = \"off\") +\n  theme_minimal() +\n  theme_epl(angle = 0, size = 20, size_title = 30,\n            size_subt = 18, size_caption = 16)\n\n\n\n\nAs one can easily see, the spikes in the time series are explained by the main events of the match. Moreover, people on Twitter was reacting as the match was evolving and goals happening. But not only with the goals, for instance, when the Liverpool player Virgil van Dijk got injured and had to be replaced, the tweets went up immediately. The peak of the tweets happened in the end of the match when Liverpool scored a goal but VAR detected offside. It could have been the goal of the win for the Reds, but after some moment of euphoric, the goal was denied and the referee blew the final whistle.\n\n\n\n",
    "preview": "posts/2021-01-10-premier-league-twitter-data-visualization-with-r/eveliv.png",
    "last_modified": "2021-01-10T20:34:01+01:00",
    "input_file": {},
    "preview_width": 612,
    "preview_height": 240
  }
]
