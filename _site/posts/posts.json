[
  {
    "path": "posts/2021-02-18-land-doesnt-vote-people-do-an-application-on-2018-brazils-presidential-election/",
    "title": "<<Land doesn't vote, people do.>> - An Application on 2018 Brazil's Presidential Election",
    "description": "This post aims to create nice maps from the 2018 Brazil's Presidential Election.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {
          "www.brunovidigal.com": {}
        }
      }
    ],
    "date": "2021-02-18",
    "categories": [],
    "contents": "\nMotivation\nYou may think this post was written too late as we’re already in 2021 and we are working on the 2018 Brazil’s presidential election data. Nevertheless, it’s never late to do nice data visualizations, and I got inspiration to write this post after seeing an outstanding map made by David Zumbach. He made the code available on this Github repository and I will apply roughly the same idea (mine is a bit simpler) in the Brazilian election.\n\n\n\nPresidential Elections\nThe presidential election takes place every four years in Brazil. The last one occurred in 2018 and ended up with a winning of the candidate Bolsonaro.\n\n\n\nHowever, I am not here to talk about how Bolsonaro is (mis)leading Brazil. I am interested in visualizing the share of votes he got in the second round of the election. For more information regarding the Brazilian elections, please go visit the Tribunal Superior Eleitoral (TSE) webpage.\nData\nIn order to produce a map with the proportion of votes Bolsonaro got, we will need to pull:\nthe shape files at municipalities level from IBGE (Instituto Brasileiro de Geografia e Estatística);\nthe results of the elections from TSE;\nmapping table with municipalities codes IBGE - TSE.\nShape files\nThe shape files can be downloaded from here. Then you select Downloads \\(\\Rightarrow\\) municipio_2019 \\(\\Rightarrow\\) Brasil \\(\\Rightarrow\\) BR \\(\\Rightarrow\\) br_municipios zip file.\n\n\n\nElection results\nThe outcome of the elections are available here. The data is divided by state, so you will need to download 28 zip files (26 states + Distrito Federal + Votes from Abroad). Instead, if you want to go for a smart solution, please have a look at this post that I have written. I will walk you on how to download these data automatically from R.\n\n\n\nMapping table IBGE - TSE\nThis table is quite useful as it contains the mapping of municipalities codes from IBGE to TSE. It is stored on this Github repository.\nReading Input Data\nLet’s start by reading the mapping table from the Github repository.\n\n\ndownload.file(url = 'https://raw.githubusercontent.com/betafcc/Municipios-Brasileiros-TSE/master/municipios_brasileiros_tse.csv', \n              destfile = 'data/mapping_tse_ibge.csv')\n\nmapping_tse_ibge <- read_csv('data/mapping_tse_ibge.csv')\nmapping_tse_ibge <- mapping_tse_ibge %>% \n  select(codigo_tse, codigo_ibge) %>% \n  mutate(codigo_tse = as.character(str_pad(codigo_tse, 5, pad = '0')),\n         codigo_ibge = as.character(codigo_ibge))\n\nmapping_tse_ibge\n\n\n# A tibble: 5,570 x 2\n   codigo_tse codigo_ibge\n   <chr>      <chr>      \n 1 01120      1200013    \n 2 01570      1200054    \n 3 01058      1200104    \n 4 01007      1200138    \n 5 01015      1200179    \n 6 01074      1200203    \n 7 01112      1200252    \n 8 01139      1200302    \n 9 01104      1200328    \n10 01554      1200344    \n# … with 5,560 more rows\n\nThe shape file from IBGE is below.\n\n\nbr_shp <- st_read(\"data/br_municipios_20200807/BR_Municipios_2019.shp\")\n\n\nReading layer `BR_Municipios_2019' from data source `/home/vidigal/Documents/projects/github/brunovidigal_distill/_posts/2021-02-18-land-doesnt-vote-people-do-an-application-on-2018-brazils-presidential-election/data/br_municipios_20200807/BR_Municipios_2019.shp' using driver `ESRI Shapefile'\nSimple feature collection with 5572 features and 4 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -73.99045 ymin: -33.75118 xmax: -28.84764 ymax: 5.271841\nproj4string:    +proj=longlat +ellps=GRS80 +no_defs \n\nclass(br_shp)\n\n\n[1] \"sf\"         \"data.frame\"\n\nIf we plot the shape file above we get:\n\n\nbr_shp %>% \n  ggplot() + \n  geom_sf() +\n  theme_void() +\n  ggtitle('Brazil - Municipalities')\n\n\n\n\nThe last piece of info needed are the votes. As we saw in my previous post, the results of the elections are split by state (SG_UF), and within each file/state, the data are disaggregated by municipality (NM_MUNICIPIO) and electoral section (NR_SECAO). Let’s first get the name of all csv’s files we need to read. Instead of typing one by one …\n\n\n\nWe can use the function list.files().\n\n\ncsvs_to_read = list.files(\n  path = \"data/elections_2018\",  \n  pattern = \".*(bweb_2t).*csv$\", \n  recursive = TRUE,          \n  full.names = TRUE         \n)\n\nhead(csvs_to_read)\n\n\n[1] \"data/elections_2018/BWEB_2t_AC_301020181744/bweb_2t_AC_301020181744.csv\"\n[2] \"data/elections_2018/BWEB_2t_AL_301020181744/bweb_2t_AL_301020181745.csv\"\n[3] \"data/elections_2018/BWEB_2t_AM_301020181744/bweb_2t_AM_301020181745.csv\"\n[4] \"data/elections_2018/BWEB_2t_AP_301020181744/bweb_2t_AP_301020181746.csv\"\n[5] \"data/elections_2018/BWEB_2t_BA_301020181744/bweb_2t_BA_301020181746.csv\"\n[6] \"data/elections_2018/BWEB_2t_CE_301020181744/bweb_2t_CE_301020181747.csv\"\n\nNow we’re ready to go ahead and read all those files. Another amazing function that we’re going to use is map_df() from purrr. Here I am using also fread() from package data.table. Combining both tidyverse and data.table solutions minimize as much as possible the time to read and combine the files into one single object of class tbl_df.\n\n\nelections_2nd_round <- \n  csvs_to_read %>% \n  map_df(~fread(., colClasses = 'character', showProgress = TRUE)) %>% \n  as_tibble()\n\n\n\nThe final object has 2904174 rows and 42 columns.\nLet’s see how our data looks like. As I am biased I will filter my hometown Rio Pomba. My city has about 18,000 habitants and is located in the countryside of the state of Minas Gerais. We’re famous about our cheese and doce de leite (milk jam or dulce de leche in Spanish). But here we go, we select Rio Pomba and a random electoral section. Mind that the data found contain not only the number of votes for each candidate, but also the number of null and blank. In this analysis, we’re interested in analysing only the valid votes.\n\n\nstr(elections_2nd_round %>% \n  filter(NM_MUNICIPIO == 'RIO POMBA' & NR_SECAO == 20))\n\n\ntibble [8 × 42] (S3: tbl_df/tbl/data.frame)\n $ DT_GERACAO                : chr [1:8] \"30/10/2018\" \"30/10/2018\" \"30/10/2018\" \"30/10/2018\" ...\n $ HH_GERACAO                : chr [1:8] \"17:49:29\" \"17:49:29\" \"17:49:29\" \"17:49:29\" ...\n $ ANO_ELEICAO               : chr [1:8] \"2018\" \"2018\" \"2018\" \"2018\" ...\n $ CD_PLEITO                 : chr [1:8] \"229\" \"229\" \"229\" \"229\" ...\n $ DT_PLEITO                 : chr [1:8] \"28/10/2018\" \"28/10/2018\" \"28/10/2018\" \"28/10/2018\" ...\n $ NR_TURNO                  : chr [1:8] \"2\" \"2\" \"2\" \"2\" ...\n $ CD_ELEICAO                : chr [1:8] \"296\" \"296\" \"296\" \"296\" ...\n $ DS_ELEICAO                : chr [1:8] \"Elei\\xe7\\xe3o Geral Federal 2018\" \"Elei\\xe7\\xe3o Geral Federal 2018\" \"Elei\\xe7\\xe3o Geral Federal 2018\" \"Elei\\xe7\\xe3o Geral Federal 2018\" ...\n $ DT_ELEICAO                : chr [1:8] \"28/10/2018\" \"28/10/2018\" \"28/10/2018\" \"28/10/2018\" ...\n $ SG_ UF                    : chr [1:8] \"MG\" \"MG\" \"MG\" \"MG\" ...\n $ CD_MUNICIPIO              : chr [1:8] \"51152\" \"51152\" \"51152\" \"51152\" ...\n $ NM_MUNICIPIO              : chr [1:8] \"RIO POMBA\" \"RIO POMBA\" \"RIO POMBA\" \"RIO POMBA\" ...\n $ NR_ZONA                   : chr [1:8] \"239\" \"239\" \"239\" \"239\" ...\n $ NR_SECAO                  : chr [1:8] \"20\" \"20\" \"20\" \"20\" ...\n $ NR_LOCAL_VOTACAO          : chr [1:8] \"1015\" \"1015\" \"1015\" \"1015\" ...\n $ CD_CARGO_PERGUNTA         : chr [1:8] \"1\" \"1\" \"1\" \"1\" ...\n $ DS_CARGO_PERGUNTA         : chr [1:8] \"Presidente\" \"Presidente\" \"Presidente\" \"Presidente\" ...\n $ NR_PARTIDO                : chr [1:8] \"#NULO#\" \"#NULO#\" \"13\" \"17\" ...\n $ SG_PARTIDO                : chr [1:8] \"#NULO#\" \"#NULO#\" \"PT\" \"PSL\" ...\n $ NM_PARTIDO                : chr [1:8] \"#NULO#\" \"#NULO#\" \"Partido dos Trabalhadores\" \"Partido Social Liberal\" ...\n $ QT_APTOS                  : chr [1:8] \"304\" \"304\" \"304\" \"304\" ...\n $ QT_COMPARECIMENTO         : chr [1:8] \"226\" \"226\" \"226\" \"226\" ...\n $ QT_ABSTENCOES             : chr [1:8] \"78\" \"78\" \"78\" \"78\" ...\n $ CD_TIPO_URNA              : chr [1:8] \"1\" \"1\" \"1\" \"1\" ...\n $ DS_TIPO_URNA              : chr [1:8] \"Apurada\" \"Apurada\" \"Apurada\" \"Apurada\" ...\n $ CD_TIPO_VOTAVEL           : chr [1:8] \"2\" \"3\" \"1\" \"1\" ...\n $ DS_TIPO_VOTAVEL           : chr [1:8] \"Branco\" \"Nulo\" \"Nominal\" \"Nominal\" ...\n $ NR_VOTAVEL                : chr [1:8] \"95\" \"96\" \"13\" \"17\" ...\n $ NM_VOTAVEL                : chr [1:8] \"Branco\" \"Nulo\" \"FERNANDO HADDAD\" \"JAIR BOLSONARO\" ...\n $ QT_VOTOS                  : chr [1:8] \"6\" \"20\" \"91\" \"109\" ...\n $ NR_URNA_EFETIVADA         : chr [1:8] \"1144507\" \"1144507\" \"1144507\" \"1144507\" ...\n $ CD_CARGA_1_URNA_EFETIVADA : chr [1:8] \"711.509.856.566.808.007.\" \"711.509.856.566.808.007.\" \"711.509.856.566.808.007.\" \"711.509.856.566.808.007.\" ...\n $ CD_CARGA_2_URNA_EFETIVADA : chr [1:8] \"218.832\" \"218.832\" \"218.832\" \"218.832\" ...\n $ CD_FLASCARD_URNA_EFETIVADA: chr [1:8] \"2685D0FE\" \"2685D0FE\" \"2685D0FE\" \"2685D0FE\" ...\n $ DT_CARGA_URNA_EFETIVADA   : chr [1:8] \"23/09/2018 08:19:00\" \"23/09/2018 08:19:00\" \"23/09/2018 08:19:00\" \"23/09/2018 08:19:00\" ...\n $ DS_CARGO_PERGUNTA_SECAO   : chr [1:8] \"1 - 20\" \"1 - 20\" \"1 - 20\" \"1 - 20\" ...\n $ DS_AGREGADAS              : chr [1:8] \"#NULO#\" \"#NULO#\" \"#NULO#\" \"#NULO#\" ...\n $ DT_ABERTURA               : chr [1:8] \"28/10/2018 08:00:00\" \"28/10/2018 08:00:00\" \"28/10/2018 08:00:00\" \"28/10/2018 08:00:00\" ...\n $ DT_ENCERRAMENTO           : chr [1:8] \"28/10/2018 17:00:58\" \"28/10/2018 17:00:58\" \"28/10/2018 17:00:58\" \"28/10/2018 17:00:58\" ...\n $ QT_ELEITORES_BIOMETRIA_NH : chr [1:8] \"3\" \"3\" \"3\" \"3\" ...\n $ NR_JUNTA_APURADORA        : chr [1:8] \"#NULO#\" \"#NULO#\" \"#NULO#\" \"#NULO#\" ...\n $ NR_TURMA_APURADORA        : chr [1:8] \"#NULO#\" \"#NULO#\" \"#NULO#\" \"#NULO#\" ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\nAs you may be thinking, we need to:\naggregate the data at municipality level;\nfilter only valid votes for president;\ncalculate Bolsonaro’s share (% of votes for Bolsonaro);\nget the municipality code at IBGE code by merging with the mapping table.\nThe piece of code below does this for us.\n\n\nvotes_by_city <- elections_2nd_round %>% \n  filter(!is.na(as.numeric(QT_VOTOS))) %>% \n  group_by(NM_MUNICIPIO, CD_MUNICIPIO, `SG_ UF`, NM_VOTAVEL, DS_CARGO_PERGUNTA) %>% \n  summarise(votes_total = sum(as.numeric(QT_VOTOS))) %>% \n  filter(DS_CARGO_PERGUNTA == 'Presidente' & `SG_ UF` != 'ZZ') %>% ## ZZ means votes abroad \n  spread(NM_VOTAVEL, votes_total) %>% \n  mutate(valid_votes = `FERNANDO HADDAD` + `JAIR BOLSONARO`,\n         perc_bolsonaro = `JAIR BOLSONARO`/valid_votes) %>% \n  left_join(mapping_tse_ibge, by = c(\"CD_MUNICIPIO\" = \"codigo_tse\")) %>% \n  ungroup() %>% \n  select(codigo_ibge, valid_votes, perc_bolsonaro)\n\nvotes_by_city\n\n\n# A tibble: 5,570 x 3\n   codigo_ibge valid_votes perc_bolsonaro\n   <chr>             <dbl>          <dbl>\n 1 2100055           53238          0.515\n 2 5200050            6213          0.690\n 3 3100104            3938          0.538\n 4 5200100            6985          0.745\n 5 3100203           11870          0.639\n 6 1500107           81542          0.244\n 7 2300101            5346          0.127\n 8 2900207            9530          0.218\n 9 2900108            4436          0.393\n10 4100103            4513          0.608\n# … with 5,560 more rows\n\nCategorizing our variable …\n\n\n# Recoding Bolsonaro's votes\nvotes_by_city <- votes_by_city %>% \n  mutate(\n    perc_bolsonaro_cat = factor(case_when(\n      perc_bolsonaro < 0.35 ~ \"\",\n      perc_bolsonaro >= 0.35 & perc_bolsonaro < 0.40 ~ \"35\", \n      perc_bolsonaro >= 0.40 & perc_bolsonaro < 0.45 ~ \"40\",\n      perc_bolsonaro >= 0.45 & perc_bolsonaro < 0.50 ~ \"45\",\n      perc_bolsonaro >= 0.50 & perc_bolsonaro < 0.55 ~ \"50\",\n      perc_bolsonaro >= 0.55 & perc_bolsonaro < 0.60 ~ \"55\",\n      perc_bolsonaro >= 0.60 & perc_bolsonaro < 0.65 ~ \"60\",\n      perc_bolsonaro >= 0.65 ~ \"65\"\n    ), levels = c(\"\", \"35\", \"40\", \"45\", \"50\", \"55\", \"60\", \"65\")\n    )\n  )\n\n\n\nData Merging\n\n\nvotes_by_city_shp <- br_shp %>% \n  left_join(votes_by_city, by = c(\"CD_MUN\" = \"codigo_ibge\")) %>% \n  filter(!is.na(valid_votes))\n\n\n\nMap\nHere we go, the map of votes at municipality level.\n\n\nggplot(data = votes_by_city_shp, aes(geometry = geometry)) +\n  geom_sf(aes(fill = perc_bolsonaro)) +\n  scale_fill_binned(low = \"#c91022\", high = \"#1a7bc5\", labels = percent) +\n  # scale_fill_viridis_c(option = \"plasma\", trans = \"sqrt\", labels = percent) +\n  theme_void() +\n  labs(title = \"Percentage Votes for Bolsonaro\",\n       subtitle = \"2nd round - Presidential Elections 2018\",\n       caption = \"@vidigal_br\") +\n  theme(legend.title = element_blank()) +\n  # theme(legend.key.size = unit(0.2, \"cm\")) +\n  theme(plot.title = element_text(size = 30, face = \"bold\", family = 'calibri')) +\n  theme(plot.subtitle = element_text(size = 20, face = \"bold\", family = 'calibri')) +\n  theme(plot.caption = element_text(size = 15, face = \"bold\", family = 'calibri'))\n\n\n\n\nThe map of the categorical variable looks like this …\n\n\nggplot(votes_by_city_shp$geometry) +\n  geom_sf(aes(fill = votes_by_city_shp$perc_bolsonaro_cat), color = NA) +\n  coord_sf() +\n  scale_fill_manual(\n    values = c(\n      \"#8d0613\", \"#c91022\", \"#f1434a\", \"#ff9193\",\n      \"#91cdff\", \"#42a2f1\", \"#1a7bc5\", \"#105182\"\n    ),\n    drop = F,\n    guide = guide_legend(\n      direction = \"horizontal\",\n      keyheight = unit(2, units = \"mm\"),\n      keywidth = unit(c(25, rep(7, 6), 25), units = \"mm\"),\n      title.position = \"top\",\n      title.hjust = 0.5,\n      label.hjust = 1,\n      nrow = 1,\n      byrow = T,\n      reverse = T,\n      label.position = \"bottom\",\n    )\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\") +\n  theme(plot.title = element_text(size = 30, face = \"bold\", family = 'calibri')) +\n  theme(plot.subtitle = element_text(size = 20, face = \"bold\", family = 'calibri')) +\n  theme(plot.caption = element_text(size = 15, face = \"bold\", family = 'calibri')) +\n  theme(legend.text = element_text(size = 15, face = \"bold\", family = 'calibri')) +\n    labs(title = \"Percentage Votes for Bolsonaro\",\n       subtitle = \"2nd round - Presidential Elections 2018\",\n       caption = \"@vidigal_br\") +\n  theme(legend.title = element_blank())\n\n\n\n\nHowever, as land doesn’t vote … people do, we will make another map taking into account the number of valid votes per each municipality. This piece of code comes from David Zumbach, I have just adapted to my own data/variables.\n\n\n# Prep function inputs (specify radius factor)\nradii <- votes_by_city_shp %>% \n  # filter(mun_id %in% start$id) %>% \n  select(CD_MUN, valid_votes) %>% \n  mutate(radius = sqrt(3000*valid_votes / pi)) %>% \n  arrange(CD_MUN) %>% \n  pull(radius) \n\nids <- votes_by_city_shp$CD_MUN\n\n# Transformation from Polygons to circles\n\n# Function to draw circles \ndraw_circle <- function(id, centre_x = 0, centre_y = 0, radius = 1000, detail = 360, st = TRUE) {\n  \n  i <- seq(0, 2 * pi, length.out = detail + 1)[-detail - 1]\n  x <- centre_x + (radius * sin(i))\n  y <- centre_y + (radius * cos(i))\n  \n  if (st) {\n    \n    cir <- st_polygon(list(cbind(x, y)[c(seq_len(detail), 1), , drop = FALSE]))\n    d <- st_sf(data.frame(id = id, geom = st_sfc(cir)))\n    \n  } else {\n    \n    d <- tibble(id = id, x = x, y = y)\n    \n  }\n  \n  return(d)\n  \n}\n\n# Draw circles\n# centroids <- as_tibble(st_coordinates(st_centroid(mapa_mg)))\ncentroids <- st_transform(votes_by_city_shp$geometry, 29101) %>% \n  st_centroid() %>% \n  # this is the crs from d, which has no EPSG code:\n  # st_transform(., '+proj=longlat +ellps=GRS80 +no_defs') %>%\n  # since you want the centroids in a second geometry col:\n  st_coordinates() %>% \n  as_tibble()\n\nclass(centroids)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nperc_bolsonaro <- votes_by_city_shp$perc_bolsonaro\n\nend <- pmap_dfr(list(ids, centroids$X, centroids$Y, radii), draw_circle)\n\n## merge\n\nend_votes <- end %>% \n  left_join(votes_by_city, by = c(\"id\" = \"codigo_ibge\")) \n\n\n\nNow we’re ready to plot the map with circles weighted by the number of valid votes. Big cities like São Paulo and Rio de Janeiro will be better represented.\n\n\nggplot(end_votes) +\n  geom_sf(aes(fill = perc_bolsonaro_cat), color = NA) +\n  coord_sf() +\n  scale_fill_manual(\n    values = c(\n      \"#8d0613\", \"#c91022\", \"#f1434a\", \"#ff9193\",\n      \"#91cdff\", \"#42a2f1\", \"#1a7bc5\", \"#105182\"\n    ),\n    drop = F,\n    guide = guide_legend(\n      direction = \"horizontal\",\n      keyheight = unit(2, units = \"mm\"),\n      keywidth = unit(c(25, rep(7, 6), 25), units = \"mm\"),\n      title.position = \"top\",\n      title.hjust = 0.5,\n      label.hjust = 1,\n      nrow = 1,\n      byrow = T,\n      reverse = T,\n      label.position = \"bottom\",\n    )\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\") +\n    labs(title = \"Percentage Votes for Bolsonaro\",\n       subtitle = \"2nd round - Presidential Elections 2018\",\n       caption = \"@vidigal_br\") +\n  theme(legend.title = element_blank()) +\n  theme(plot.title = element_text(size = 30, face = \"bold\", family = 'calibri')) +\n  theme(plot.subtitle = element_text(size = 20, face = \"bold\", family = 'calibri')) +\n  theme(plot.caption = element_text(size = 15, face = \"bold\", family = 'calibri')) +\n  theme(legend.text = element_text(size = 15, face = \"bold\", family = 'calibri')) \n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-18-land-doesnt-vote-people-do-an-application-on-2018-brazils-presidential-election/brazil.png",
    "last_modified": "2021-02-18T21:04:53+01:00",
    "input_file": {},
    "preview_width": 843,
    "preview_height": 744
  },
  {
    "path": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/",
    "title": "Web Scraping Brazil's Presidential Election Data",
    "description": "This post shows how to scrape Brazil's Presidential Election Data from TSE.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {}
      }
    ],
    "date": "2021-01-09",
    "categories": [],
    "contents": "\n\nContents\nMotivation\nView page source\nR codeWeb scraping\nRead in data\n\nConclusion\n\nMotivation\nI won’t lie to you. The very first time I downloaded elections data from the Tribunal Superior Eleitoral TSE was manually … and painful.\n28 files to download and unzip … MANUALLY!\nBut it was also clear to me that this procedure wasn’t the most recommended. I should figure out a better way to do that.\nThus, looking for a simple way to solve this problem I found this question on stackoverflow:\n\n\n\nAnd Hadley answered it with class.\n\n\n\nWith this valuable info, I needed to:\nFind the downloadable link as per the question;\nApply&Adapt Hadley’s solution using rvest package.\nView page source\nIn order to find the downloadable link, see the gif below or check this youtube video as the resolution is higher. The tip is to look for the downloadable link right clicking on the View page source button.\n\n\n\nR code\nThe R packages used are:\n\n\nlibrary(rvest)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(data.table)\nlibrary(stringr)\n\n\n\nWeb scraping\nThe R code below is pretty much what Hadley had posted. I have just adapted it to my problem. Then the object page receives the html file we saw in the video above. The function read_html() from package xml2 just reads the html. After that we apply html_nodes() to find the links, html_attr() to get the url and str_subset() to find the files ending in .zip and excluding the ones ending in .sha.\n\n\npage <- xml2::read_html(\"https://www.tse.jus.br/hotsites/pesquisas-eleitorais/resultados_anos/boletim_urna/2018/boletim_urna_2_turno.html\")\n\nzip_files <- page %>%\n  html_nodes(\"a\") %>%       # find all links\n  html_attr(\"href\") %>%     # get the url\n  str_subset(\"\\\\.zip\") %>% # find those that end in .zip\n  str_subset(\"\\\\.sha\", negate = TRUE) # rid of the ones ending in .sha\n\n\n\nOnce you have run this code above, you download those files, unzip and save them in your machine.\n\n\nfor(i in seq_along(zip_files)) {\n  \n  temp <- tempfile()\n  download.file(zip_files[i], temp)\n  unzip(temp, exdir = \"data/elections_2018\")\n  unlink(temp)\n  \n}\n\n\n\nRead in data\nAs we are lazy (or should I say smart enough), let’s list all data at once with the function list.files().\n\n\ncsvs_to_read = list.files(\n  path = \"data/elections_2018\",  \n  pattern = \".*(bweb_2t).*csv$\", \n  recursive = TRUE,          \n  full.names = TRUE         \n)\n\n\n\nThat done, you can use the fantastic R function map_df() from purrr coupled with fread() from data.table.\n\n\nelections_2nd_round <- \n  csvs_to_read %>% \n  map_df(~fread(., colClasses = 'character', showProgress = TRUE)) %>% \n  as_tibble()\n\n\n\nIn a few seconds you get your data (nearly 3 million rows and 1,3GB) ready to be analysed.\nConclusion\nThat’s all folks. Pretty simple to web scraping Brazil’s Presidential Election data.\n\n\n\n",
    "preview": "posts/2021-01-09-web-scraping-brazils-presidential-election-data/images/stackoverflow_question.png",
    "last_modified": "2021-01-10T18:27:52+01:00",
    "input_file": {},
    "preview_width": 957,
    "preview_height": 914
  },
  {
    "path": "posts/2021-01-10-premier-league-twitter-data-visualization-with-r/",
    "title": "Premier League: Twitter Data Visualization with R",
    "description": "From tweets to data visualization in R.",
    "author": [
      {
        "name": "Bruno Vidigal",
        "url": {
          "www.brunovidigal.com": {}
        }
      }
    ],
    "date": "2020-10-26",
    "categories": [],
    "contents": "\n\nContents\nSynopsis\nData collection: rtweet\nData Manipulation\nData Visualization\n\nSynopsis\nLiverpool and Everton met on Saturday 17 October 2020 for the fifth round of the English Premier League (EPL), the most competitive and thrilling national tournament of the world - it’s just the best. As a good and traditional derby, the teams brought a lot of rivalry to the pitch and as result, a 2-2 draw, full of polemics and drama. As I am passionate about both statistics and football/soccer (the beautiful game), I decided to analyze what was going on on Twitter during the match. To do so, I used the great R package rtweet to pull tweets with the hashtags #EVELIV and #MerseySideDerby.\nData collection: rtweet\nThere are many tutorials on the web about how to pull Twitter data with rtweet. For instance, you can have a look at:\ntowards data science\nEarth Data Science\nMedium\nAs there are many good articles on Internet, I will assume you know how to get your Twitter API access and will just show you the parameters used to get the tweets.\nFirstly, you you’ll need to load some R packages.\n\n\n# load packages -----------------------------------------------------------\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(magick)\n\n\n\nAfter loading the packages you will need to specify the authentication token’s provided in your Twitter App with the function create_token(). After that you can use the function search_tweets() to get your data.\n\n\ncreate_token(\n  app = \"Liverpool tweets\",\n  consumer_key = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  consumer_secret = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  access_token = \"xxxxxxxxxxxxxxxxxxxxxxx\",\n  access_secret = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\ntweets <- search_tweets(\"EVELIV OR MerseysideDerby\", n = 100000, \n                                    include_rts = FALSE, lang = \"en\", retryonratelimit = TRUE)\n\n\n\nData Manipulation\nAlthough I have put n equals to 100,000, I got about 40k rows (39982), which I assume is the number of tweets with these hashtags. Another step towards our aim is to filter out some columns. The API gives you 90 columns. In this exercise we’re interested in some of them.\n\n\ntweets <- tweets %>% select(user_id, status_id, created_at, screen_name, text, \n                            display_text_width, is_quote, is_retweet, favorite_count,\n                            retweet_count, quote_count, hashtags, mentions_screen_name)\n\n\n\nBear in mind that the time shown in the Twitter API is set to UTC. The game took place in Liverpool (GMT +1) and started at 12:30 pm (local time), 11:30 am (UTC). We will consider the first and the last tweet of the Liverpool Football Club as the start (11:31:49 am UTC) and the end (13:28:15 UTC) time of the match. Using mutate() and case_when() we create a new variable moment with three categories: pre-game, game and post-game. Here we’re interested in just the moment of the match.\n\n\ngame_started_at <- as.POSIXct('2020-10-17 11:31:49', tz = 'UTC')\ngame_ended_at <- as.POSIXct('2020-10-17 13:28:15', tz = 'UTC')\n\ntweets <- tweets %>% \n  mutate(\n    moment = case_when(\n      created_at >= game_started_at & created_at <= game_ended_at ~ \"game\",\n      created_at > game_ended_at ~ \"post-game\", \n      TRUE ~ \"pre-game\"))\n\n\n\nData Visualization\nWe load the Premier League and Twitter logos to make the graph looks nicer. In this exercise I am using the package magick, function image_read().\n\n\npremier_league_logo = image_read(\"premier_league_logo.jpg\")\ntwitter_logo <- image_read(\"Twitter_bird_logo.png\")\n\n\n\nNow we’re ready to plot the time series of the number of tweets with the hashtags #EVERLIV and #MerseySideDerby during the match. However, let’s first create the graphic without customizing it. Thus, we’ll see how good is the graphic with the proper logos and colors. This is the best way to value how important is customization. As statisticians/data analysts/data scientists, we need to know how to present a good graphic to an end user/client.\n\n\n  tweets %>% \n  filter(moment == \"game\") %>%\n  ggplot(aes(created_at)) + \n  geom_freqpoly(binwidth = 60) \n\n\n\n\nAnd now we make the same graphic adding the Premier League colors, logos and the main events with annotate(). Go check also the Michael Toth blog for more tips on how to brand your graphs.\n\n\nsource('theme_epl.R') # customize theme for English Premier League\n\n  tweets %>% \n  filter(moment == \"game\") %>%\n  ggplot(aes(created_at)) + \n  geom_freqpoly(binwidth = 60, size = 1.2, col = '#3d195b') +\n  xlab('Time of the match - UTC') + ylab('') +  \n  scale_y_continuous(labels = comma) +\n  labs(title = \"#tweets (#EVELIV and #MerseysideDerby) over Everton vs Liverpool\", \n       subtitle = \"English Premier League - 5th round, 17th October 2020\",\n       caption = \"@vidigal_br\", \n       family = \"sans\") + \n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), y = 1100, \n           label = \"1st Goal \\n Sadio Mane\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:42:00\", tz = 'UTC'), y = 30, \n           label = \"Virgil van Dijk \\n replaced\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), y = 1100, \n           label = \"1st Goal \\n Michael Keane\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), y = 800, \n           label = \"2nd Goal \\n Salah\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), y = 1100, \n           label = \"2nd Goal \\n Calvert-Lewin\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), y = 30, \n           label = \"Red card \\n Richarlisson\", colour = '#274488', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), y = 1650, \n           label = \"VAR: NO GOAL \\n Offside\", colour = '#D00027', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 12:28:00\", tz = 'UTC'), y = 800, \n           label = \"Interval\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 11:55:00\", tz = 'UTC'), y = 1600, \n           label = \"1st Half\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\"text\", x = as.POSIXct(\"2020-10-17 13:05:00\", tz = 'UTC'), y = 1600, \n           label = \"2nd Half\", colour = '#3d195b', size = 8, family = \"sans\") +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), y = 1000, \n    xend = as.POSIXct(\"2020-10-17 11:35:00\", tz = 'UTC'), yend = 450, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:40:00\", tz = 'UTC'), y = 100,\n    xend = as.POSIXct(\"2020-10-17 11:40:00\", tz = 'UTC'), yend = 300, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), y = 1000,\n    xend = as.POSIXct(\"2020-10-17 11:51:00\", tz = 'UTC'), yend = 480, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), y = 650, \n    xend = as.POSIXct(\"2020-10-17 13:03:00\", tz = 'UTC'), yend = 480, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), y = 950,\n    xend = as.POSIXct(\"2020-10-17 13:11:57\", tz = 'UTC'), yend = 650, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), y = 150,\n    xend = as.POSIXct(\"2020-10-17 13:20:59\", tz = 'UTC'), yend = 300, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"segment\", x = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), y = 1550,\n    xend = as.POSIXct(\"2020-10-17 13:25:10\", tz = 'UTC'), yend = 1450, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) + \n  geom_vline(xintercept = as.POSIXct(\"2020-10-17 12:21:08\", tz = 'UTC'), linetype=\"dotted\", \n             color = \"#3d195b\", size=1.5) + \n  geom_vline(xintercept = as.POSIXct(\"2020-10-17 12:36:07\", tz = 'UTC'), linetype=\"dotted\", \n             color = \"#3d195b\", size=1.5) +\n  annotation_custom(rasterGrob(premier_league_logo),\n                    xmin = as.POSIXct(\"2020-10-17 11:15:00\", tz = 'UTC'), \n                    xmax = as.POSIXct(\"2020-10-17 11:25:00\", tz = 'UTC'), \n                    ymin = -100, ymax = -250) +\n  annotation_custom(rasterGrob(twitter_logo),\n                    xmin = as.POSIXct(\"2020-10-17 13:20:00\", tz = 'UTC'), \n                    xmax = as.POSIXct(\"2020-10-17 13:25:00\", tz = 'UTC'), \n                    ymin = -250, ymax = -120) +\n  coord_cartesian(clip = \"off\") +\n  theme_minimal() +\n  theme_epl(angle = 0, size = 20, size_title = 30,\n            size_subt = 18, size_caption = 16)\n\n\n\n\nAs one can easily see, the spikes in the time series are explained by the main events of the match. Moreover, people on Twitter was reacting as the match was evolving and goals happening. But not only with the goals, for instance, when the Liverpool player Virgil van Dijk got injured and had to be replaced, the tweets went up immediately. The peak of the tweets happened in the end of the match when Liverpool scored a goal but VAR detected offside. It could have been the goal of the win for the Reds, but after some moment of euphoric, the goal was denied and the referee blew the final whistle.\n\n\n\n",
    "preview": "posts/2021-01-10-premier-league-twitter-data-visualization-with-r/eveliv.png",
    "last_modified": "2021-01-10T20:34:01+01:00",
    "input_file": {},
    "preview_width": 612,
    "preview_height": 240
  }
]
